P. 31

### Chapter 1 Exercises, P. 10
1. How do you know if a code base is an application? A release?
The presence of a relx.config file or a relx tuple in rebar.config.

2. What differentiates an application from a library application?
A library application is neither started nor stopped.

3. What can be said of processes under a one_for_all scheme for supervision?
The processes under a one_for_all scheme entirely depend on each other.

4. Why would someone use a gen_fsm behaviour over a gen_server?
When reason someone would use a gen_fsm over a gen_server is if the functionalty
happens in a particular sequence. If a sequence is not required, then a
gen_server implementation could help parallelize work.

1. Is this application meant to be used as a library? A standalone system?
This application is not meant to be used as a library- the app file defines
`mod`.

It seems to be intended to be standalone b/c it has a `relx` tuple in
`rebar.config`, therefore it's a release and releases are standalone (I think).

2. What does it do?
Starts a supervisor which supervisors a number of gen_servers as listed in the
application file. Each gen_server generates a random number which seems to be
used to determine the frequency at which each gen_server will log a random
String- ostensibly to simulate a person speaking.

3. Does it have any dependencies? What are they?
Yes, it has dependencies. The dependencies are gproc and recon.

4. The app’s README mentions being non-deterministic. Can you prove if this is
true? How?
The general logic for the app is based on the number generated in
council_member:init/1 via rand:uniform(10). So, the README should be correct
in that assertion.

5. Can you express the dependency chain of applications in there? Generate a
diagram of them?
Council depends on crypto and gproc. Council, as noted in the generated diagram
by being represented as an oval is an application, gproc and crypto (rectangles)
as library applications. Recon is a dependency as well and is also a library
application.

6. Can you add more processes to the main application than those described in
the README?
Yes, more processes can be added by adding a String to the list of process names
in the application file.

### Chapter 2, Section 2.3 Exercises, P. 18
1. Are Erlang supervision trees started depth-first? breadth-first?
Synchronously or asynchronously?
Erlang supervision trees are started synchronously depth-first.

2. What are the three application strategies? What do they do?
  • permanent: if the app terminates, the entire system is taken down, excluding
               manual termination of the app with application:stop/1.
  • transient: if the app terminates for reason normal, that’s ok. Any other
               reason for termination shuts down the entire system.
  • temporary: the application is allowed to stop for any reason. It will be
               reported, but nothing bad will happen.

3. What are the main differences between the directory structure of an app and a
release?
An OTP application can be expected to have one top-level supervisor (if any)
and possibly a bunch of dependencies that sit below it. An OTP release will
usually be composed of multiple OTP applications, which may or may not depend on
each other.

Instead of having a top-level app alone in src, applications can be nested one
level deeper in a apps or lib directory...This structure lends itself to
generating releases where multiple OTP applications under your control under a
single code repository.

4. When should you use a release?
If what you’re writing is a stand-alone piece of code that could be used by
someone building a product, it’s likely an OTP application. If what you’re
building is a product that stands on its own and should be deployed by users
as-is (or with a little configuration), what you should be building is an OTP
release.

5. Give two examples of the type of state that can go in a process’ init
function, and two examples of the type of state that shouldn’t go in a process’
init function.

Should (stuff you can guarantee):
- configuration files
- access to the file system

Should not (stuff you cannot guarantee):
- non-local database
- external services

Using the code at https://github.com/ferd/recon_demo:
1. Extract the main application hosted in the release to make it independent,
and includable in other projects.

2. Host the application somewhere (Github, Bitbucket, local server), and build a
release with that application as a dependency.

3. The main application’s workers (council_member) starts a server and connects
to it in its init/1 function. Can you make this connection happen outside of the
init function’s? Is there a benefit to doing so in this specific case?

Chapter 3 Exercises, P. 31

1. Name the common sources of overload in Erlang systems
- In a default Erlang install, the error_logger2 process will take its sweet time to log things... especially true of
  user-generated log messages... and for crashes in large processes
- Locking and blocking operations
- Unexpected messages...Messages you didn’t know about tend to be rather rare when using OTP...However, all kinds of
  OTP-compliant systems end up having processes that may not implement a behaviour, or processes that go in a
  non-behaviour stretch where it overtakes message handling

2. What are the two main classes of strategies to handle overload?
Slowing things down and dropping things.

- Slow down (back-pressure)
  -  Restricting Input...The most common way to restrict data input is to make calls to a process whose queue
  would grow in uncontrollable ways synchronously.
  - Asking For Permission...A somewhat simpler approach to back-pressure is to identify the resources we want to block
  on, those that cannot be made faster and are critical to your business and users. Lock these
  resources behind a module or procedure where a caller must ask for the right to make a
  request and use them. 

- Dropping (load-shedding)
  - random drops
  - stack, queue, and/or time-sensitive buffers

3. How can long-running operations be made safer?
- Transform blocking tasks into a asynchronous tasks
- When there is any point of your program that ends up being a central hub for receiving
messages, lengthy tasks should be moved out of there if possible
- Handling predictable overload situations by adding more processes — which either handle the blocking operations
or instead act as a buffer while the "main" process blocks — is often a good idea.
- Start the long-running job and keep a token that identifies it uniquely, along with the original requester you’re
doing work for. When the resource is available, have it send a message back to the server with the aforementioned token.
The server will eventually get the message, match the token to the requester, and answer back, without being blocked by
other requests in the mean time

4. When going synchronous, how should timeouts be chosen?
- What’s particularly tricky about applying back-pressure to handle overload via synchronous
calls is having to determine what the typical operation should be taking in terms of time,
or rather, at what point the system should time out.
The best way to express the problem is that the first timer to be started will be at
the edge of the system, but the critical operations will be happening deep within it. This
means that the timer at the edge of the system will need to have a longer wait time that
those within, unless you plan on having operations reported as timing out at the edge even
though they succeeded internally

5. What is an alternative to having timeouts?
- inifinite timeouts or asking for permission, discarding data, buffering of various types

6. When would you pick a queue buffer before a stack buffer?
- A major downside of stack buffers is that messages are not necessarily going
  to be processed in the order they were submitted — they’re nicer for
  independent tasks, but will ruin your day if you expect a sequence of events
  to be respected.

Open-ended Questions
1. What is a true bottleneck? How can you find it?

2. In an application that calls a third party API, response times vary by a lot
   depending on how healthy the other servers are. How could one design the system
   to prevent occasionally slow requests from blocking other concurrent calls to
   the same service?

3. What’s likely to happen to new requests to an overloaded latency-sensitive
   service where data has backed up in a stack buffer? What about old requests?

4. Explain how you could turn a load-shedding overload mechanism into one that
   can also provide back-pressure.

5. Explain how you could turn a back-pressure mechanism into a load-shedding
   mechanism.

6. What are the risks, for a user, when dropping or blocking a request? How can
   we prevent duplicate messages or missed ones?

7. What can you expect to happen to your API design if you forget to deal with
   overload, and suddenly need to add back-pressure or load-shedding to it?

